# -*- coding: utf-8 -*-
"""marketing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aapmeQlCdjpvo-DafT6r9icC3jzYZd9X
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.compose import ColumnTransformer
import warnings
warnings.filterwarnings("ignore")
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, KFold
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
import joblib

df = pd.read_csv('marketing_campaign.csv', sep='\t')

print(df.head())

print(df.info())
print(df.describe())
print(df.isnull().sum())
print(df.columns)

num_cols = df.select_dtypes(include=np.number).columns
for col in num_cols:
    plt.figure(figsize=(6,4))
    sns.histplot(df[col].dropna(), kde=True, bins=30)
    plt.title(f'Distribution of {col}')
    plt.xlabel(col)
    plt.ylabel('Frequency')
    plt.show()

cat_cols = df.select_dtypes(exclude=np.number).columns
for col in cat_cols:
    plt.figure(figsize=(6,4))
    sns.countplot(y=df[col], order=df[col].value_counts().index)
    plt.title(f'Countplot of {col}')
    plt.show()

top_corr_features = corr.abs().unstack().sort_values(ascending=False)
top_corr_pairs = top_corr_features[(top_corr_features < 1)].head(3).index
pair_cols = list(set([i for sub in top_corr_pairs for i in sub]))

if len(pair_cols) > 1:
    sns.pairplot(df[pair_cols].dropna())
    plt.show()

for col in num_cols:
    plt.figure(figsize=(6,4))
    sns.boxplot(x=df[col])
    plt.title(f'Boxplot of {col}')
    plt.show()

if 'Dosage' in df.columns and 'Volume' in df.columns:
    df['Dosage_to_Volume_Ratio'] = df['Dosage'] / df['Volume']
else:
    print("Columns 'Dosage' and/or 'Volume' not found — skipping this feature.")

if 'Temperature' in df.columns:
    def temp_band(temp):
        if temp < 10:
            return 'Low'
        elif 10 <= temp < 25:
            return 'Medium'
        else:
            return 'High'

    df['Temperature_Band'] = df['Temperature'].apply(temp_band)
else:
    print("Column 'Temperature' not found — skipping temperature banding.")

if 'Age' in df.columns:
    df['Age_Group'] = pd.cut(df['Age'], bins=[0, 25, 45, 65, 100],
                               labels=['Young', 'Adult', 'Mature', 'Senior'])
else:
    print("Column 'Age' not found — skipping age grouping.")

label_encoder = LabelEncoder()
df['Gender'] = label_encoder.fit_transform(df['Gender'])
categorical_cols = df.select_dtypes(include=['object']).columns
df_encoded = pd.get_dummies(df, columns=categorical_cols, drop_first=True)
print(df_encoded.head())

candidates = ['Sales','Response','AmountSpent','TotalSpent','Income','y','target','PurchaseAmount','Revenue']
target = None
for c in candidates:
    if c in df.columns:
        target = c; break
if target is None:
    numeric_cols_tmp = df.select_dtypes(include=[np.number]).columns.tolist()
    if len(numeric_cols_tmp)==0:
        raise ValueError("No numeric columns found to use as target. Specify target column.")
    target = df[numeric_cols_tmp].var().sort_values(ascending=False).index[0]

X = df.drop(columns=[target]); y = df[target]
num_cols = X.select_dtypes(include=[np.number]).columns.tolist()
cat_cols = X.select_dtypes(include=['object','category','bool']).columns.tolist()

numeric_transformer = Pipeline([('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler())])
categorical_transformer = Pipeline([('imputer', SimpleImputer(strategy='constant', fill_value='missing')), ('onehot', OneHotEncoder(handle_unknown='ignore'))])

preprocessor = ColumnTransformer([('num', numeric_transformer, num_cols), ('cat', categorical_transformer, cat_cols)], remainder='drop')

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

def evaluate_model(estimator, name):
    pipe = Pipeline([('preprocessor', preprocessor), ('model', estimator)])
    pipe.fit(X_train, y_train)
    preds = pipe.predict(X_test)
    rmse_test = np.sqrt(mean_squared_error(y_test, preds))
    cv = KFold(n_splits=5, shuffle=True, random_state=42)
    cv_scores = cross_val_score(pipe, X_train, y_train, scoring='neg_mean_squared_error', cv=cv, n_jobs=-1)
    cv_rmse = np.sqrt(-cv_scores)
    print(f"{name} -> Test RMSE: {rmse_test:.4f} | CV RMSE mean: {cv_rmse.mean():.4f} (std {cv_rmse.std():.4f})")
    return pipe

lr_pipe = evaluate_model(LinearRegression(), "LinearRegression")
rf_pipe = evaluate_model(RandomForestRegressor(random_state=42, n_jobs=-1), "RandomForest (default)")
gb_pipe = evaluate_model(GradientBoostingRegressor(random_state=42), "GradientBoosting (default)")

param_grid_rf = {'model__n_estimators':[100,200],'model__max_depth':[None,10,20],'model__min_samples_split':[2,5]}
pipe_rf = Pipeline([('preprocessor', preprocessor), ('model', RandomForestRegressor(random_state=42))])
gs_rf = GridSearchCV(pipe_rf, param_grid_rf, scoring='neg_mean_squared_error', cv=3, n_jobs=-1)
gs_rf.fit(X_train, y_train)
print("RF best params:", gs_rf.best_params_, "CV RMSE:", np.sqrt(-gs_rf.best_score_))

param_grid_gb = {'model__n_estimators':[100,200],'model__learning_rate':[0.05,0.1],'model__max_depth':[3,5]}
pipe_gb = Pipeline([('preprocessor', preprocessor), ('model', GradientBoostingRegressor(random_state=42))])
gs_gb = GridSearchCV(pipe_gb, param_grid_gb, scoring='neg_mean_squared_error', cv=3, n_jobs=-1)
gs_gb.fit(X_train, y_train)
print("GB best params:", gs_gb.best_params_, "CV RMSE:", np.sqrt(-gs_gb.best_score_))

best_rf = gs_rf.best_estimator_
best_gb = gs_gb.best_estimator_
for name, model in [("RF tuned", best_rf), ("GB tuned", best_gb)]:
    preds = model.predict(X_test)
    print(name, "Test RMSE:", np.sqrt(mean_squared_error(y_test, preds)))

joblib.dump(best_rf, "/content/best_random_forest_pipeline.joblib")
joblib.dump(best_gb, "/content/best_gradient_boosting_pipeline.joblib")
print("Models saved to /content/")
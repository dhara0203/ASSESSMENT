# -*- coding: utf-8 -*-
"""Manufacturing: Anomaly Detection in Engines.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xoaO6oP7S-PoT_QblGKnrOG5yFVAl9VM
"""

import os
import io
import zipfile
import requests
from pathlib import Path

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import DBSCAN
import seaborn as sns

from sklearn.ensemble import IsolationForest

url = "https://github.com/bhaumik2025/DS_Data/raw/main/Datasets/Unsupervised%20ML/NASA%20Turbofan%20Jet%20Engine%20Data%20Set.zip"

print("Downloading dataset...")
response = requests.get(url)
response.raise_for_status()

with zipfile.ZipFile(io.BytesIO(response.content)) as z:
    z.extractall("turbofan_data")

print("‚úÖ Dataset extracted to folder: turbofan_data")

import os
for root, dirs, files in os.walk("turbofan_data"):
    for f in files:
        print(os.path.join(root, f))

file_path = "turbofan_data/CMaps/train_FD001.txt"

data = pd.read_csv(file_path, sep=r"\s+", header=None)

columns = (
    ["unit_id", "time_in_cycles"] +
    [f"operational_setting_{i}" for i in range(1, 4)] +
    [f"sensor_measurement_{i}" for i in range(1, 22)]
)
data.columns = columns

print("üîπ Total Units (Engines):", data["unit_id"].nunique())
print("üîπ Example Unit IDs:", data["unit_id"].unique()[:10])

cycles_per_unit = data.groupby("unit_id")["time_in_cycles"].max()
print("\nüî∏ Number of cycles (time steps) for first 5 engines:")
print(cycles_per_unit.head())

sensor_cols = [col for col in data.columns if "sensor" in col]
print("\nüîπ Total Sensors:", len(sensor_cols))
print("üîπ Sensor Columns:", sensor_cols)

print("\nüìä First few sensor readings:")
print(data[sensor_cols[:5]].head())

print("\nüìà Sensor Summary Statistics:")
print(data[sensor_cols].describe().T.head())

columns = (
    ["unit_id", "time_in_cycles"] +
    [f"operational_setting_{i}" for i in range(1, 4)] +
    [f"sensor_measurement_{i}" for i in range(1, 22)]
)
data.columns = columns

print("‚úÖ Data loaded successfully.")
print("Shape:", data.shape)
print("Columns:", data.columns.tolist()[:10], "...")

print("\nüîç Missing Values:")
print(data.isnull().sum().sum(), "total missing values")
data = data.fillna(method="ffill")

constant_sensors = [col for col in data.columns if "sensor" in col and data[col].std() == 0]
print("\n‚öôÔ∏è Constant sensors (to drop):", constant_sensors)

data = data.drop(columns=constant_sensors)

sensor_cols = [col for col in data.columns if "sensor" in col]
scaler = MinMaxScaler(feature_range=(0, 1))
data[sensor_cols] = scaler.fit_transform(data[sensor_cols])

print("\nüìè Sensor data normalized to range [0, 1]")

agg_per_unit = data.groupby("unit_id").agg({
    "time_in_cycles": ["min", "max", "count"],
    **{col: ["mean", "std", "min", "max"] for col in data.columns if "sensor" in col}
})

print("‚úÖ Aggregated summary per engine:")
print(agg_per_unit.head())

sensor_cols = [col for col in data.columns if "sensor" in col]
for col in sensor_cols[:5]:  # just first 5 sensors for demo
    data[f"{col}_rolling_mean"] = data.groupby("unit_id")[col].transform(lambda x: x.rolling(window=5, min_periods=1).mean())

print("\n‚úÖ Added 5-cycle rolling mean features for first 5 sensors.")

sensor_cols = [col for col in data.columns if "sensor" in col]
X = data[sensor_cols]
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

print("‚úÖ Sensor data standardized (mean=0, std=1).")

pca = PCA(n_components=0.95)
X_pca = pca.fit_transform(X_scaled)

print(f"‚úÖ PCA applied. Original dimensions: {X.shape[1]}, Reduced to: {X_pca.shape[1]} components")

explained_var = np.cumsum(pca.explained_variance_ratio_) * 100

plt.figure(figsize=(8,5))
plt.plot(range(1, len(explained_var)+1), explained_var, marker='o')
plt.title('Cumulative Explained Variance by PCA Components')
plt.xlabel('Number of Principal Components')
plt.ylabel('Explained Variance (%)')
plt.grid(True)
plt.show()

pca_cols = [f"PCA_Component_{i+1}" for i in range(X_pca.shape[1])]
pca_df = pd.DataFrame(X_pca, columns=pca_cols)
pca_df["unit_id"] = data["unit_id"]
pca_df["time_in_cycles"] = data["time_in_cycles"]

print("\n‚úÖ PCA DataFrame created:")
print(pca_df.head())

pca = PCA(n_components=2)  # use 2D for visualization
X_pca = pca.fit_transform(X_scaled)

print("‚úÖ PCA reduced to 2 components for clustering.")

dbscan = DBSCAN(eps=0.5, min_samples=10)
db_labels = dbscan.fit_predict(X_pca)

pca_df = pd.DataFrame(X_pca, columns=['PCA1', 'PCA2'])
pca_df["Cluster"] = db_labels
pca_df["unit_id"] = data["unit_id"]
pca_df["time_in_cycles"] = data["time_in_cycles"]

print("\nüìä Cluster label counts:")
print(pd.Series(db_labels).value_counts())

plt.figure(figsize=(8,6))
plt.scatter(pca_df["PCA1"], pca_df["PCA2"],
            c=pca_df["Cluster"], cmap="plasma", s=5)
plt.title("DBSCAN Clustering on PCA-reduced Sensor Data")
plt.xlabel("PCA Component 1")
plt.ylabel("PCA Component 2")
plt.colorbar(label="Cluster / Anomaly Label")
plt.show()

plt.figure(figsize=(10, 7))
sns.scatterplot(
    data=pca_df,
    x='PCA1', y='PCA2',
    hue='Cluster',
    palette='Spectral',
    s=15,
    alpha=0.7
)

plt.title("üîπ DBSCAN Clusters on PCA-Reduced NASA Turbofan Sensor Data", fontsize=14)
plt.xlabel("PCA Component 1")
plt.ylabel("PCA Component 2")
plt.legend(title="Cluster", bbox_to_anchor=(1.05, 1), loc='upper left')
plt.grid(True)
plt.show()

scaler = StandardScaler()
X_scaled = scaler.fit_transform(data[sensor_cols])

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)

iso_forest = IsolationForest(
    n_estimators=200,
    contamination=0.02,
    random_state=42
)
iso_labels = iso_forest.fit_predict(X_pca)

iso_df = pd.DataFrame(X_pca, columns=['PCA1', 'PCA2'])
iso_df["Anomaly_IF"] = iso_labels
iso_df["unit_id"] = data["unit_id"]
iso_df["time_in_cycles"] = data["time_in_cycles"]

plt.figure(figsize=(10,7))
normal = iso_df[iso_df["Anomaly_IF"] == 1]
anomaly = iso_df[iso_df["Anomaly_IF"] == -1]

plt.scatter(normal["PCA1"], normal["PCA2"], s=10, color='blue', label='Normal')
plt.scatter(anomaly["PCA1"], anomaly["PCA2"], s=10, color='red', label='Anomaly')

plt.title("‚öôÔ∏è Isolation Forest on PCA-reduced Sensor Data")
plt.xlabel("PCA Component 1")
plt.ylabel("PCA Component 2")
plt.legend()
plt.grid(True)
plt.show()

early_warnings = (
    iso_df[iso_df["Anomaly_IF"] == -1]
    .groupby("unit_id")["time_in_cycles"]
    .min()
    .reset_index()
    .rename(columns={"time_in_cycles": "first_anomaly_cycle"})
)

print("üîß Early warning cycles for first few engines:")
print(early_warnings.head())

total_cycles = iso_df.groupby("unit_id")["time_in_cycles"].max().reset_index()
early_warnings = early_warnings.merge(total_cycles, on="unit_id")
early_warnings["cycles_before_failure"] = (
    early_warnings["time_in_cycles"] - early_warnings["first_anomaly_cycle"]
)

print("\nüìä Cycles before failure (first 5 engines):")
print(early_warnings.head())

plt.figure(figsize=(10,7))

for uid in [1, 2, 3, 4, 5]:
    engine_data = iso_df[iso_df["unit_id"] == uid]
    plt.plot(engine_data["time_in_cycles"], engine_data["Anomaly_IF"],
             label=f"Engine {uid}", alpha=0.7)

plt.title("‚ö†Ô∏è Anomaly Signals Over Time (Isolation Forest)")
plt.xlabel("Cycle Number")
plt.ylabel("Anomaly Flag (1=Normal, -1=Anomaly)")
plt.legend()
plt.grid(True)
plt.show()

plt.figure(figsize=(10,6))
plt.bar(early_warnings["unit_id"], early_warnings["cycles_before_failure"], color='orange')
plt.xlabel("Engine ID")
plt.ylabel("Cycles Remaining After First Anomaly")
plt.title("üõ´ Remaining Useful Life (RUL) after First Anomaly Detected")
plt.grid(True)
plt.show()

dbscan = DBSCAN(eps=0.5, min_samples=10)
db_labels = dbscan.fit_predict(X_pca)
iso_forest = IsolationForest(
    n_estimators=200, contamination=0.02, random_state=42
)
iso_labels = iso_forest.fit_predict(X_pca)
data["DBSCAN_Label"] = db_labels       # -1 = anomaly
data["IForest_Label"] = iso_labels     # -1 = anomaly

# Mark unified "is_outlier" column (if any method marks it as anomaly)
data["is_outlier"] = data.apply(lambda row:
                                1 if (row.DBSCAN_Label == -1 or row.IForest_Label == -1)
                                else 0, axis=1)

print("‚úÖ Outlier labeling complete!")
print(data[["unit_id", "time_in_cycles", "DBSCAN_Label", "IForest_Label", "is_outlier"]].head())

print("\nüìä Total outliers detected:")
print(data["is_outlier"].value_counts())

normal_data = data[data["is_outlier"] == 0]
anomaly_data = data[data["is_outlier"] == 1]

print(f"‚úÖ Normal points: {len(normal_data)}, Anomalies: {len(anomaly_data)}")

sensor_stats = pd.DataFrame({
    "Normal_Mean": normal_data[sensor_cols].mean(),
    "Anomaly_Mean": anomaly_data[sensor_cols].mean()
})

sensor_stats["Difference"] = sensor_stats["Anomaly_Mean"] - sensor_stats["Normal_Mean"]

print("\nüìä Top 5 sensors with largest deviation (potential root causes):")
print(sensor_stats["Difference"].abs().sort_values(ascending=False).head())

plt.figure(figsize=(12,6))
sns.barplot(x=sensor_stats.index, y=sensor_stats["Difference"], palette="coolwarm")
plt.xticks(rotation=90)
plt.title("üìâ Sensor-Level Differences (Anomalous vs Normal Mean Values)")
plt.ylabel("Mean Difference (Anomaly - Normal)")
plt.xlabel("Sensor")
plt.grid(True)
plt.show()

top_sensors = sensor_stats["Difference"].abs().sort_values(ascending=False).head(3).index

plt.figure(figsize=(10,5))
for sensor in top_sensors:
    sns.kdeplot(normal_data[sensor], label=f"{sensor} (Normal)", fill=True, alpha=0.4)
    sns.kdeplot(anomaly_data[sensor], label=f"{sensor} (Anomaly)", fill=True, alpha=0.4)
    plt.title(f"Sensor Distribution: {sensor}")
    plt.legend()
    plt.show()